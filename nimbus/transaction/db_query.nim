# Nimbus - Steps towards a fast and small Ethereum data store
#
# Copyright (c) 2021-2022 Status Research & Development GmbH
# Licensed under either of
#  * Apache License, version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)
#  * MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)
# at your option. This file may not be copied, modified, or distributed except according to those terms.

{.push raises: [Defect].}

import
  sets, tables, memfiles,
  stint, chronicles, stew/byteutils,
  eth/common/eth_types,
  ../constants,
  ./host_types

# The purposes of the present format and very simple query methods here is to
# be a read-only proof of concept of very compact Eth account/storage state
# history, to show that it's _sufficiently_ compact to be interesting.
#
# The files it reads are generated by extracting the state history from an
# Erigon node in a separate ad-hoc ETL process.
#
# ----------------------------------------------------------------------------
#
# `memfiles.nim` stdlib module very helpfully provides memory-mapped files
# for POSIX and Windows.  (Thanks Zahary!)
#
# IMPORTANT: `mmap` is used here for this PoC but not intended to remain.
#
# This comment is to preempt anticipated feedback about the LMDB issues
# mentioned in `nimbus/db/select_backend.nim`.
#
# It is very convenient for this read-only proof of concept, while the
# database is built up in Nim layers.  Certainly it simplifies the query
# code, making it easier to explain and show the compression and traversal,
# compared with a version using high-performance async file I/O.
#
# But actually single-threaded `mmap` is a very slow method for this type of
# database.  Memory-mapped files don't have appropriate performance
# characteristics, not to mention portability issues.  I measured 25 times
# slower doing random-access reads via `mmap` like this compared with good
# async I/O on my Linux test system.
#
# Specifically, memory-mapped files perform badly with single-threaded code
# doing random-access reads to a very large database.  Because the state file
# is much larger than RAM, most queries block in a page fault doing I/O to
# the underlying storage device.  Most queries are located randomly due to
# Ethereum hashed trie keys, and are different from one Ethereum block to the
# next, so OS filesystem read-ahead just makes performance worse.  Those
# reads block Chronos in a page fault, and it's not possible for other async
# tasks to progress while waiting for storage.  Worse, it's not possible to
# reach I/O queue depth > 1 this way, so the underlying filesystem and the
# SSD itself operate at their very slowest.
#
# ----------------------------------------------------------------------------
#
# The format is a large, read-only file which is easy to search, but that's a
# little misleading.
#
# To those who know what makes a database work (B-trees/LSM-trees etc), it may
# be clear that the format here can't be updated fast.  Really though, it's not
# far off a format that supports updates while keeping good query performance
# and similar compression.  Watch this space if you're intrigued about how that
# transformation is done!
#
# This is being implemented in tested steps into a compressed (for Eth) but
# _writable_ data format, that supports "O(1)"[*] and 1 IOP state queries, fast
# bulk-range writes for super-snap sync, and fast random-writes for execution.
# Offline experiments have been done on Mainnet data to validate the concepts
# and to benchmark low-level I/O performance.  Also don't pay too close
# attention to the ad-hoc byte coding in this file.  It's scaffolding while
# we're importing data.
#
# [*] Because other clients' literature says "O(1)" queries.  Technically all
# of them are at least O(log N) because they mean O(1) database lookups, and a
# single database lookup is O(log N) reads by itself.  It may be reduced to
# O(1) under certain cache scaling assumptions and well-designed caching of
# interior pages.  We count "leaf IOPS" as a reasonably proportional proxy for
# IOPS under these assumptions.
#
# On a more concrete note, our method, when cached, takes approximately 1 leaf
# IOPS to read both a small account and N `SLOAD` storage reads, due to
# locality in the layout of small accounts.  1+N for large accounts (those with
# large storage or history).  Contrast 1 and 1+N with 2+2N and 2+2N leaf IOPS
# are used by TurboGeth-derived clients.
#
# So our method should be faster at random-read queries in an absolute sense
# when SSD I/O performance is the bottleneck, rather than CPU.  Our method is
# more CPU intensive, due to compression not layout (they could be separated).

type
  EthDB* = ref object
    memFile:      MemFile
    queries:      uint64
    queryPagesL1: uint64
    queryPagesL2: uint64

  DbHeader = object
    fileVersion:  uint64
    fileSize:     uint64
    startOffset:  uint64
    pageShift:    uint64

  DbAccount* = object
    nonce*:       AccountNonce
    incarnation*: uint64
    balance*:     UInt256
    codeHash*:    Hash256

  DbStorage = object
    slot*:        UInt256
    value*:       UInt256

  DbSlotResult* = object
    value*:       UInt256

  DbGeneralKeyBits = enum
    DbkHasBlockNumber
    DbkHasAddress
    #DbHasAddressHash
    DbkHasSlot
    #DbHasSlotHash

  DbGeneralKeyFlags = set[DbGeneralKeyBits]

  DbGeneralKey = object
    flags:        DbGeneralKeyFlags
    blockNumber:  BlockNumber
    address:      EthAddress
    #addressHash:  Hash256
    slot:         UInt256
    #slotHash:     Hash256

  DbGeneralValueBits = enum
    DbvHasBlockNumber
    DbvHasAddress
    DbvHasAccount
    DbvHasStorage

  DbGeneralValueFlags = set[DbGeneralValueBits]

  DbGeneralValue = object
    flags:        DbGeneralValueFlags
    blockNumber:  BlockNumber
    address:      EthAddress
    account:      DbAccount
    storage:      DbStorage

  DbMatch = enum
    # Polarity: Whether a key is EQ, GT or LT than a looked up value.
    MatchEQ       # Key equals value.
    MatchGT       # Key strictly greater than value.
    MatchLT       # Key strictly less than value.
    MatchNotFound # No value.

template header(db: EthDB): DbHeader =
  doAssert not db.memFile.mem.isNil
  cast[ptr DbHeader](db.memFile.mem)[]

template offset(db: EthDB, offset: uint64): ptr byte =
  cast[ptr byte](db.memFile.mem) + offset

template `+`(p: ptr byte, offset: uint64): ptr byte =
  cast[ptr byte](cast[uint](p) + offset)

template inc(p: var ptr byte) = p = p + 1

template toHex(hash: Hash256): string = hash.data.toHex

proc compareAddresses(a, b: EthAddress): DbMatch {.inline.} =
  for i in 0 ..< 20:
    if a[i] < b[i]:
      return MatchLT
    elif a[i] > b[i]:
      return MatchGT
  return MatchEQ

proc compareNumbers(a, b: uint64 | BlockNumber | UInt256): DbMatch {.inline.} =
  if a < b: MatchLT elif a > b: MatchGT else: MatchEQ

proc compareBooleans(a, b: bool): DbMatch {.inline.} =
  if a == b: MatchEQ elif not a: MatchLT else: MatchGT

proc compareGeneral(key: DbGeneralKey, value: DbGeneralValue): DbMatch =
  var match = compareBooleans(DbkHasAddress in key.flags,
                              DbvHasAddress in value.flags)
  if match != MatchEQ:
    return match

  if DbkHasAddress in key.flags:
    match = compareAddresses(key.address, value.address)
    if match != MatchEQ:
      return match

  match = compareBooleans(DbkHasSlot in key.flags,
                          DbvHasStorage in value.flags)
  if match != MatchEQ:
    return match

  if DbkHasSlot in key.flags:
    match = compareNumbers(key.slot, value.storage.slot)
    if match != MatchEQ:
      return match

  match = compareBooleans(DbkHasBlockNumber in key.flags,
                          DbvHasBlockNumber in value.flags)
  if match != MatchEQ:
    return match

  if DbkHasBlockNumber in key.flags:
    match = compareNumbers(key.blockNumber, value.blockNumber)
    if match != MatchEQ:
      return match

  return match

proc queryPage(db: EthDB, offsetStart, offsetEnd: uint64, key: DbGeneralKey,
               valueOut: var DbGeneralValue, all: bool): DbMatch =
  var
    posEnd = db.offset(offsetEnd)
    pos = db.offset(offsetStart)
    b: byte

  template getByte =
    if pos > posEnd:
      break
    b = pos[]
    inc pos

  template readFixed64(into: var uint64) =
    into = 0
    for i in 0 ..< 8:
      getByte
      into = (into shl 8) or b.uint64

  template readFixed256(into: var UInt256) =
    getByte
    into = b.u256
    for i in 0 ..< 31:
      getByte
      into = (into shl 8) or b.u256

  template readVariable256(into: var UInt256) =
    getByte
    if b < 224:
      into = b.u256
    else:
      var remainder = (b - 224).int
      getByte
      into = b.u256
      while remainder != 0:
        getByte
        into = (into shl 8) or b.u256
        dec remainder

  var blockNumber: uint64
  var address: EthAddress
  var readerIncarnation: uint64
  var readerSlot: UInt256

  const
    CODE_BLOCK_NUMBER = 1   # Range 1..8
    CODE_ADDRESS      = 9   # Single value 9
    CODE_ACCOUNT      = 10  # Range 10..73
    CODE_STORAGE      = 74  # Range 74..249
    CODE_INCARNATION  = 250 # Single value 250
    CODE_BLOCK_INLINE = 251 # Range 251..255

  template nextEntry(generalValue: var DbGeneralValue) =
    while true:
      getByte
      if b < CODE_BLOCK_NUMBER:
        # End of page
        break
      elif b <= CODE_BLOCK_NUMBER + 7:
        let len = (b - CODE_BLOCK_NUMBER + 1).int
        blockNumber = 0
        for i in 0 ..< len:
          getByte
          blockNumber = (blockNumber shl 8) or b.uint64
      elif b == CODE_ADDRESS:
        for i in 0 ..< 20:
          getByte
          address[i] = b

      elif b <= CODE_ACCOUNT + 63:
        generalValue.flags = { DbvHasBlockNumber, DbvHasAddress, DbvHasAccount }
        generalValue.blockNumber = blockNumber.toBlockNumber
        generalValue.address = address
        template account: var DbAccount = generalValue.account
        account.nonce = 0
        account.incarnation = 0
        account.balance = 0.u256
        account.codeHash = ZERO_HASH256

        let flags = b - CODE_ACCOUNT

        if (flags and 1) != 0:
          readVariable256(account.balance)

        if (flags and 2) != 0:
          for i in 0 ..< 32:
            getByte
            account.codeHash.data[i] = b

        if (flags and (3 shl 2)) == (3 shl 2):
          readFixed64(account.nonce)
        else:
          account.nonce = ((flags shr 2) and 3).AccountNonce

        if (flags and (3 shl 4)) == (3 shl 4):
          readFixed64(account.incarnation)
        else:
          account.incarnation = ((flags shr 2) and 3).uint64
        if account.incarnation != 0:
          readerIncarnation = account.incarnation

        # At this point we have an account entry.
        break

      elif b <= CODE_STORAGE + 160 + 15:
        generalValue.flags = { DbvHasBlockNumber, DbvHasAddress, DbvHasStorage }
        generalValue.blockNumber = blockNumber.toBlockNumber
        generalValue.address = address
        template storage: var DbStorage = generalValue.storage

        let flags = b - CODE_STORAGE

        if (flags shr 4) < 9:
          storage.slot = (flags shr 4).u256
        elif (flags shr 4) == 9:
          readVariable256(storage.slot)
        else:
          readFixed256(storage.slot)
        if (flags and (1 shl 3)) != 0:
          storage.slot += readerSlot
        readerSlot = storage.slot

        if (flags and 7) < 6:
          storage.value = (flags and 7).u256
        else:
          readVariable256(storage.value)
          if (flags and 7) == 6:
            storage.value = not storage.value

        # At this point we have a storage entry.
        break

      elif b <= CODE_INCARNATION + 4:
        if b - CODE_INCARNATION < 4:
          readerIncarnation = (b - CODE_INCARNATION + 1).uint64
        else:
          readFixed64(readerIncarnation)

      else:
        trace "DB: Syntax error in data file"
        break

  var haveSavedValue: bool
  var savedValue: DbGeneralValue
  var match = MatchNotFound

  while true:
    valueOut.flags = {}
    nextEntry(valueOut)
    match = compareGeneral(key, valueOut)
    if match != MatchLT or not all:
      break
    savedValue = valueOut
    haveSavedValue = true

  if match == MatchGT and haveSavedValue:
    valueOut = savedValue
    match = MatchLT

  return match

template checkHeader(db: EthDB) =
  doAssert db.memFile.mem != nil
  doAssert db.memFile.size >= sizeof(db.header)
  doAssert db.header.fileVersion == 2022020701
  doAssert db.header.fileSize <= db.memFile.size.uint64
  doAssert db.header.fileSize >= db.header.startOffset
  doAssert db.header.pageShift >= 8 and db.header.pageShift <= 24

proc generalQuery(db: EthDB, key: DbGeneralKey,
                  valueOut: var DbGeneralValue): bool =
  db.checkHeader()
  let
    fileSize = db.header.fileSize
    pageShift = db.header.pageShift
    pageMask = ((1 shl pageShift) - 1).uint64

  inc db.queries

  var
    lowOffset = db.header.startOffset
    highOffset = fileSize - 1
    value {.noinit.}: DbGeneralValue

  # The key we are looking for is between the lowest key in page
  # `lowOffset shr pageShift` and the highest key in page
  # `highOffset shr pageShift`, both inclusive.
  #
  # We hunt for it with two levels of binary search.  First using "search in
  # page, first key only" as a subroutine to locate the page.  Then using
  # "search in page, all keys" to locate the value.
  #
  # A subtle twist is that the query has mixed comparators.  We're not looking
  # for an exact match on all key components.  We need an exact match to
  # `address` and `slot`, but for `blockNumber` we search for the "max entry <=
  # search key", because this is really an interval tree with implicit
  # intervals (block ranges).  To find the entry, the second level search may
  # re-visit a page that was discarded early in the first level search.

  while true:
    if lowOffset > highOffset:
      return false

    var
      # Subtraction like this biases the rounding upwards, which reduces the
      # average number of search steps and page reads.
      midOffset = highOffset - ((highOffset - lowOffset) shr 1)
      midPageStart = midOffset shr pageShift
      midPageEnd = midPageStart or pageMask

    # For "max entry <= search key" it is always better to skip the first page
    # at L1 search if there is another page after (unless the first entry
    # happens to `MatchEQ`, which is unlikely).
    if midPageStart <= lowOffset:
      if midPageEnd >= highOffset:
        break
      midPageStart += pageMask + 1
      midPageEnd = midPageStart or pageMask

    if midPageEnd > highOffset:
      midPageEnd = highOffset

    inc db.queryPagesL1
    case queryPage(db, midPageStart, midPageEnd, key, valueOut, false):
      of MatchEQ:
        return true
      of MatchLT:
        highOffset = midPageStart - 1
      of MatchGT:
        # The "max entry <= search key" might be inside the current page, but
        # might also be in a much higher page number.  We can set `lowOffset`
        # to either the start of the current page, or the start of the next one
        # and be prepared to backtrack one page.  Each has a subtle effect on
        # the average number of steps in L1 search.  The choice below is made
        # to interact with skipping the first page in code above, because
        # skipping the first page is beneficial at the first iteration as well.
        lowOffset = midPageStart
      of MatchNotFound:
        return false

  inc db.queryPagesL2
  case queryPage(db, lowOffset, highOffset, key, valueOut, true):
    of MatchEQ:
      return true
    of MatchGT:
      # Match exact address and slot but "lowest >=" block number.
      if compareBooleans(DbkHasAddress in key.flags,
                         DbvHasAddress in value.flags) != MatchEQ:
        return false
      if DbkHasAddress in key.flags:
        if compareAddresses(key.address, valueOut.address) != MatchEQ:
          return false
      if compareBooleans(DbkHasSlot in key.flags,
                         DbvHasStorage in value.flags) != MatchEQ:
        return false
      if DbkHasSlot in key.flags:
        if compareNumbers(key.slot, valueOut.storage.slot) != MatchEQ:
          return false
      return true
    else:
      return false

proc ethDbQueryAccount*(db: EthDB, blockNumber: BlockNumber,
                      address: EthAddress, accountResult: var DbAccount): bool =
  var key {.noinit.}: DbGeneralKey
  key.flags = { DbkHasBlockNumber, DbkHasAddress }
  key.blockNumber = blockNumber
  key.address = address

  var value {.noinit.}: DbGeneralValue
  let found = generalQuery(db, key, value)
  if not found:
    accountResult = DbAccount()
  else:
    accountResult = value.account
  return found

proc ethDbQueryStorage*(db: EthDB, blockNumber: BlockNumber,
                        address: EthAddress, slot: UInt256,
                        slotResult: var DbSlotResult): bool =
  var key {.noinit.}: DbGeneralKey
  key.flags = { DbkHasBlockNumber, DbkHasAddress, DbkHasSlot }
  key.blockNumber = blockNumber
  key.address = address
  key.slot = slot

  var value {.noinit.}: DbGeneralValue
  let found = generalQuery(db, key, value)
  if not found:
    slotResult.value = 0.u256
  else:
    slotResult.value = value.storage.value
  return found

proc ethDbOpen*(path: string): EthDB {.raises: [IOError, OSError].} =
  ## Open an EthDB file.  Note, format is subject to rapid change.
  ## See comment at the start of this file about use of `memfile`.
  info "DB: Opening experimental compressed state history database",
    file=path
  let db = EthDB()
  # Raises `OSError` on error, good enough for this versipn.
  result.memFile = memfiles.open(path)

proc ethDbSize*(db: EthDB): uint64 =
  db.checkHeader()
  return db.header.fileSize

proc ethDbShowStats*(db: EthDb) =
  let queryPages = db.queryPagesL1 + db.queryPagesL2
  debug "DB: Statistics so far", queries=db.queries,
    pagesPerQuery=(queryPages.float / db.queries.float), queryPages,
    queryPagesL1=db.queryPagesL1, queryPagesL1=db.queryPagesL2
